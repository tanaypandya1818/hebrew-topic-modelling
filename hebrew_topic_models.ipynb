{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724e4aee",
   "metadata": {},
   "source": [
    "# Topic Modeling on Genesis in Hebrew (Masoretic Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c7b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/Documents/NLP_NEU/hebrew_topic_modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliasmann/Documents/NLP_NEU/hebrew_topic_modeling/hebrew_tokenizer/tokenizer.py:121: FutureWarning: Possible nested set at position 843\n",
      "  self.scanner = re.compile(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, HdpModel, LsiModel\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from hebrew_tokenizer.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import NMF\n",
    "from coherence.coherence_scores import compute_coherence_score_umass, compute_coherence_score_uci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151390a3-2c19-47e4-92d2-501113ffab78",
   "metadata": {},
   "source": [
    "## Preprocessing and Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6c5d34-bd3b-4498-afa9-55c1d1a906d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add current directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "file_path = 'data/genesis_hebrew.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    hebrew_text = file.read()\n",
    "    \n",
    "# Tokenize the Hebrew text\n",
    "tokenizer = Tokenizer()\n",
    "tokens = list(tokenizer.tokenize(hebrew_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fee8d02-d0f0-4e23-8808-177d228acf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokenized documents\n",
    "documents = [[word[1] for word in tokenizer.tokenize(line)] for line in hebrew_text.split('\\n')]\n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4b8b4",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb6b4282-fe3f-4c18-937f-ea0fa73b0fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['son', 'all', 'which', 'country', 'was', 'Come on', 'said', 'Nathan', 'god', 'Lesson']\n",
      "1 ['said', 'son', 'was', 'sleep', 'Boy', 'god', 'Because', 'woman', 'Gone', 'No']\n",
      "2 ['said', 'which', 'brother', 'son', 'please', 'Yosef', 'was', 'all', 'Come on', 'No']\n",
      "3 ['Ob', 'Because', 'was', 'all', 'country', 'See', 'said', 'read', 'Pharaoh', 'No']\n",
      "4 ['said', 'which', 'Because', 'country', 'man', 'Jehovah', 'all', 'No', 'Come on', 'action']\n",
      "0 ['בֵּן', 'כֹּל', 'אֲשֶׁר', 'אֶרֶץ', 'היה', 'בּוא', 'אמר', 'נתן', 'אֱלֹהִים', 'לקח']\n",
      "1 ['אמר', 'בֵּן', 'היה', 'שָׁנָה', 'ילד', 'אֱלֹהִים', 'כִּי', 'אִשָּׁה', 'הלךְ', 'לֹא']\n",
      "2 ['אמר', 'אֲשֶׁר', 'אָח', 'בֵּן', 'נָא', 'יוֺסֵף', 'היה', 'כֹּל', 'בּוא', 'לֹא']\n",
      "3 ['אָב', 'כִּי', 'היה', 'כֹּל', 'אֶרֶץ', 'ראה', 'אמר', 'קרא', 'פַּרְעֹה', 'לֹא']\n",
      "4 ['אמר', 'אֲשֶׁר', 'כִּי', 'אֶרֶץ', 'אִישׁ', 'יהוה', 'כֹּל', 'לֹא', 'בּוא', 'עשׂה']\n"
     ]
    }
   ],
   "source": [
    "# Set the number of topics\n",
    "num_topics = 5\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Print the top 5 tokens with highest probability for each topic\n",
    "topics = lda_model.show_topics(num_words=10, formatted=False)\n",
    "for idx, topic in lda_model.show_topics(num_words=10, formatted=False):\n",
    "    print(idx, [GoogleTranslator(source='auto', target='en').translate(word[0])\n",
    "                    for word in topic])\n",
    "    \n",
    "for idx, topic in lda_model.show_topics(num_words=10, formatted=False):\n",
    "    print(idx, [word[0] for word in topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0692ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -91.67991199488442\n",
      "Average Coherence Score UCI: 2.1692099877272732\n"
     ]
    }
   ],
   "source": [
    "hebrew_text_lines = [[word[1] for word in tokenizer.tokenize(line)] for line in hebrew_text.split('\\n')]\n",
    "hebrew_text_flat = [token[1] for token in tokens]\n",
    "\n",
    "\n",
    "cumulative_coherence_umass = 0\n",
    "cumulative_coherence_uci = 0\n",
    "\n",
    "# Calculate coherence for each topic extracted by HDP\n",
    "for topic_num, topic in topics:\n",
    "    top_terms = [word[0] for word in topic]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, hebrew_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is similarly defined and imported\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, hebrew_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/len(topics))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d159d",
   "metadata": {},
   "source": [
    "## HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c399202-e4d6-454b-a696-3e3f2a9eed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['גּנב', 'מַקֵּל', 'אַבְרָם', 'בְּכוֺר', 'אֱמֶת', 'צִבְעוֺן', 'מֵאָה', 'נגע', 'זקן', 'בַּת', 'דּוֺר', 'רָעָה', 'חִתִּי', 'עשׂה', 'נָחָשׁ', 'בִּנְיָמִן', 'תּוֺלָדוֺת', 'אִשָּׁה', 'בּרח', 'אַךְ']\n",
      "\n",
      "Topic 1: ['הרה', 'ילד', 'נטה', 'עַתָּה', 'צחק', 'זָקֵן', 'זָכָר', 'יוֺנָה', 'חזק', 'קום', 'עֶלְיוֺן', 'קַיִן', 'נְקֵבָה', 'זֶה', 'דִּינָה', 'אֹזֶן', 'כֹּל', 'פֶּה', 'טוֺב', 'יְהוּדָה']\n",
      "\n",
      "Topic 2: ['עלה', 'תֶּרַח', 'ירד', 'נַעֲרָה', 'חוה', 'קלל', 'תּוֺלָדוֺת', 'זֶרַח', 'עוֺף', 'עֲשָׂרָה', 'שְׁמֹנֶה', 'אָז', 'ישׁב', 'שֵׁנִי', 'אָח', 'חפר', 'קַיִן', 'מוֺלֶדֶת', 'מִן', 'שֵׁת']\n",
      "\n",
      "Topic 3: ['אסף', 'אַתְּ', 'הַר', 'מְאוּמָה', 'אָב', 'שִׂמְלָה', 'שֵׁשׁ', 'יוֺסֵף', 'שְׁמֹנִים', 'אַחֵר', 'צַוָּאר', 'עֵשֶׂב', 'שׁאר', 'עשׂה', 'רחץ', 'פֶּה', 'לוֺט', 'רוּחַ', 'עזב', 'כְּנַעַן']\n",
      "\n",
      "Topic 4: ['חָם', 'טַבָּח', 'כּוֺכָב', 'רוץ', 'צָעִיר', 'מַלְאָךְ', 'יכח', 'סְדֹם', 'יקץ', 'בּרךְ', 'הרג', 'מִדְבָּר', 'בְּאֵר', 'עִם', 'פֶּלֶג', 'הֵנָּה', 'עֵדֶר', 'כַּד', 'בָּקָר', 'נשׂא']\n",
      "\n",
      "Topic 5: ['זֶרַע', 'יָם', 'מַרְאֶה', 'הרה', 'יֶרֶד', 'ילד', 'צִבְעוֺן', 'עֶשְׂרִים', 'נשׁק', 'רחץ', 'משׁשׁ', 'חפר', 'מֵאָה', 'עִם', 'אֱדוֺם', 'אַתֶּם', 'אַיִן', 'חֹרִי', 'יטב', 'אֵל']\n",
      "\n",
      "Topic 6: ['שִׁפְחָה', 'בָּשָׂר', 'מִשְׁפָּחָה', 'חַיָּה', 'אַחַת', 'שָׁם', 'נפל', 'כִּכָּר', 'צָעִיר', 'עֶפְרוֺן', 'קֶשֶׁת', 'קֶדֶם', 'הָגָר', 'אפה', 'טַבָּח', 'לְמַעַן', 'יהב', 'מכר', 'צלח', 'רְכוּשׁ']\n",
      "\n",
      "Topic 7: ['מִצְרִי', 'מלךְ', 'שַׁדַּי', 'עֶבֶד', 'רעע', 'עֶשְׂרֵה', 'עִבְרִי', 'משׁל', 'מִן', 'שְׁמֹנִים', 'אַתָּה', 'רָקִיעַ', 'יַעֲקֹב', 'אסף', 'שׁקה', 'נצב', 'יטב', 'שׁבר', 'חיה', 'ירשׁ']\n",
      "\n",
      "Topic 8: ['אמר', 'עשׂה', 'עוֺד', 'לֵב', 'אָהֳלִיבָמָה', 'פּוץ', 'אַבְרָם', 'שָׁם', 'כְּנַעַן', 'זֶה', 'גּבר', 'מלךְ', 'אֵל', 'רום', 'כְּנַעֲנִי', 'תּוֺלָדוֺת', 'עָפָר', 'אָדוֺן', 'הרג', 'זֶרַח']\n",
      "\n",
      "Topic 9: ['אֱלִיפַז', 'בּוֺר', 'נָא', 'אֲדָמָה', 'מַטְעַמִּים', 'זָכָר', 'לֶמֶךְ', 'מַחֲנֶה', 'יוֺם', 'עוֺף', 'צָעִיר', 'חֵן', 'שִׁפְחָה', 'רום', 'אַתֶּם', 'ידע', 'מַלְאָךְ', 'שֵׁנִי', 'בָּקָר', 'עָדָה']\n",
      "\n",
      "Topic 10: ['עֶרֶב', 'מכר', 'מִצְרַיִם', 'יסף', 'כּלה', 'זֶרַע', 'תֵּשַׁע', 'עֵבֶר', 'אֶפְרַיִם', 'מָה', 'בָּשְׂמַת', 'יהוה', 'אַלּוּף', 'שׁבר', 'בִּלְתִּי', 'יַחְדָּו', 'נָקֹד', 'יָלִיד', 'יַעֲקֹב', 'יֵשׁ']\n",
      "\n",
      "Topic 11: ['חרה', 'אֱמֶת', 'שֶׁלַח', 'פּתר', 'גּור', 'סור', 'אָמָה', 'משׁשׁ', 'כּוֺכָב', 'פְּרִי', 'הָגָר', 'רוּחַ', 'בַּת', 'מִקְנֶה', 'יֵשׁ', 'נכה', 'רָע', 'מול', 'פִּתְרוֺן', 'מִן']\n",
      "\n",
      "Topic 12: ['בּוֺר', 'מֵאָה', 'אָחוֺת', 'עֶשְׂרֵה', 'ירשׁ', 'בּנה', 'גֹּשֶׁן', 'פּרה', 'מות', 'הֵן', 'שֵׂעִיר', 'דִּינָה', 'רִאשׁוֺן', 'נטה', 'לֶחֶם', 'תֶּרַח', 'שְׁלֹשִׁים', 'ילד', 'חַיָּה', 'פָּנִים']\n",
      "\n",
      "Topic 13: ['קֶדֶם', 'חַי', 'אֱדוֺם', 'עֶשֶׂר', 'אֵל', 'מָקוֺם', 'משׁשׁ', 'בִּלְהָה', 'רחץ', 'רָע', 'מִשְׁפָּחָה', 'יְהוּדָה', 'שִׁבְעָה', 'קֵץ', 'יֵשׁ', 'רֹאשׁ', 'ישׁב', 'נטה', 'יִשְׁמָעֵאל', 'רַב']\n",
      "\n",
      "Topic 14: ['מול', 'בּנה', 'חוּץ', 'עֵשֶׂב', 'היה', 'אַחַר', 'שׁרץ', 'פּתר', 'אָדָם', 'רגל', 'לבשׁ', 'ירשׁ', 'בַּיִן', 'צמח', 'מָקוֺם', 'הוּא', 'שׁבר', 'מְעָרָה', 'עוֺד', 'מות']\n",
      "\n",
      "Topic 15: ['אֵצֶל', 'סבב', 'צַיִד', 'מַהֲלַלְאֵל', 'קוֺל', 'אוּלַי', 'לֶמֶךְ', 'שְׁמֹנִים', 'רֵעַ', 'ענה', 'בּרח', 'פּרד', 'עֵדֶר', 'דּבר', 'בֶּלַע', 'עבר', 'יצא', 'אוֺ', 'רעה', 'פּתח']\n",
      "\n",
      "Topic 16: ['יֶפֶת', 'שׁבר', 'עֲשָׂרָה', 'בִּלְהָה', 'אַרְבָּעִים', 'שִׁפְחָה', 'יָד', 'בֹּקֶר', 'מְנַשֶּׁה', 'דֶּרֶךְ', 'בַּד', 'רמשׂ', 'גּנב', 'רֶגֶל', 'יָמִין', 'שְׁלֹשָׁה', 'נסע', 'כּלה', 'נגשׁ', 'בּוֺר']\n",
      "\n",
      "Topic 17: ['בִּלְהָה', 'מָוֶת', 'חֲמוֺר', 'דּבר', 'שׁאב', 'אָהֳלִיבָמָה', 'אָז', 'שֶׁלַח', 'אַל', 'אֵצֶל', 'נגד', 'ענה', 'יָפֶה', 'שׁחת', 'מִצְרִי', 'אַרְבָּעִים', 'יִשְׂרָאֵל', 'צֹעַר', 'היה', 'עבד']\n",
      "\n",
      "Topic 18: ['רֶגֶל', 'שׁמע', 'אֱמֹרִי', 'מִקְנָה', 'עֵדֶר', 'בֵּן', 'זכר', 'משׁשׁ', 'נוח', 'שִׁבְעִים', 'רָע', 'ארר', 'משׁל', 'מְאֹד', 'מצא', 'אַחַר', 'לֵב', 'שָׂרָה', 'פּוץ', 'עוֺלָה']\n",
      "\n",
      "Topic 19: ['ךּ', 'אֵצֶל', 'חיה', 'יַיִן', 'עֶשְׂרִים', 'מְנַשֶּׁה', 'חֵן', 'נפל', 'זקן', 'שִׁפְחָה', 'תִּשְׁעִים', 'פָּנֶה', 'עֵז', 'גּוע', 'דָּן', 'שַׁדַּי', 'דָּם', 'עֶשְׂרֵה', 'נֹחַ', 'הֵנָּה']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the HDP model\n",
    "hdp_model = HdpModel(corpus, dictionary)\n",
    "\n",
    "#Print the topics\n",
    "topics = hdp_model.show_topics()\n",
    "\n",
    "for topic_id, topic in topics:\n",
    "    topic_words = [word[6:] for word in topic.split(' + ')]\n",
    "    print(f\"Topic {topic_id}: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00e33479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['came out', 'upper', 'Queen', 'night', 'ez', 'neck', 'Boy', 'past', 'First', 'Praise be to God', 'Blood', 'language', 'Sur', 'Location', 'Costs', 'knowledge', 'Goshen', 'Stood', 'very', 'sixty']\n",
      "\n",
      "Topic 1: ['barrel', 'rum', 'water', 'herd', 'Ten', 'bag', 'hunger', 'Sermon', 'Moved', 'Neighbor', 'carry', 'sixty', 'hay', 'Haim', 'sir', 'an old', 'killing', 'against', 'multi-', 'six']\n",
      "\n",
      "Topic 2: ['Crete', 'ninety', 'Eye', 'Rabetz', 'an animal', 'Bad', 'Appearance', 'Maybe', 'Tf', 'Weapon', 'Ree', 'Tool', 'ez', 'contract', 'More', 'Th', 'lie down', 'None', 'Manasseh', 'a dress']\n",
      "\n",
      "Topic 3: ['seed', 'food', 'Moved', 'stick', 'Boy', 'lewdness', 'now', 'Hebron', 'property', 'Tf', 'send', 'Fraction', 'battle', 'Tohor', 'Mr', 'loved', 'past', 'a girl', 'Closed', 'Curse']\n",
      "\n",
      "Topic 4: ['summer', 'Yosef', 'carry', 'Ask', 'Name', 'cow', 'which', 'Confusion', 'Rebecca', 'Oh, Eliboma', 'mortal', 'brother', 'victory', 'Closed', 'heart', 'to', 'Remesh', 'son', 'cement', 'Yes']\n",
      "\n",
      "Topic 5: ['thank you', 'ask', 'Because', 'Healthy', 'meat', 'Until', 'cerebro', 'Rachel', 'came out', 'Closed', 'Fell', 'could', 'Build', 'Benjamin', 'First', 'Two', 'mother', 'Remesh', 'head', 'now']\n",
      "\n",
      "Topic 6: ['king', 'Hello', 'prey', 'Abraham', 'Salary', 'are you', 'an animal', 'Hebrew', 'order', 'at', 'Hand', 'Shim', 'Sarah', 'Yosef', 'Ntsb', 'girl', 'Angel', 'a dress', 'you', 'Snake']\n",
      "\n",
      "Topic 7: ['very', 'laughed', 'Increased', 'Heavy', 'Manasseh', 'Flood', 'Haran', 'Pigeon', 'tilted', 'herd', 'heart', 'Grace', 'from', 'ten', 'Yosef', 'gold', 'dawned', 'Goshen', 'Neighbor', 'Lon']\n",
      "\n",
      "Topic 8: ['stone', 'brother', 'Sodom', 'Eighty', 'Sarah', 'sea', 'Amory', 'death', 'slave', 'to', 'Ask', 'Th', 'get up', 'Prd', 'shoulder', 'son', 'Tool', 'Six', 'See', 'nose']\n",
      "\n",
      "Topic 9: ['god', 'seed', 'Tamar', 'Four', 'you', 'Oh, Eliboma', 'Moderator', 'Tool', 'More', 'please', 'lewdness', 'will end', 'gold', 'Dan', 'Bdl', 'female', 'sat down', 'male', 'Ten', 'Oats']\n",
      "\n",
      "Topic 10: ['Voice', 'Come on', 'ninety', 'Egypt', 'battle', 'a cover', 'Witness', 'Costs', 'A little', 'Sarah', 'Beef', 'end', 'tilted', 'male', 'slave', 'jug', 'in fire', 'dream', 'stone', 'Bela']\n",
      "\n",
      "Topic 11: ['non', 'sky', 'grave', 'Nut', 'land', 'Jacob', 'Canaan', 'Why', 'Hello', 'Yahav', 'Eye', 'Hittite', 'Errr', 'sleep', 'Arpachshad', 'Roots', 'multi', 'Eliphaz', 'girl', 'Moved']\n",
      "\n",
      "Topic 12: ['Ten', 'past', 'Why', 'Run away', 'Come on', 'Beef', 'get up', 'Tamar', 'was', 'sword', 'animal', 'Ptr', 'point', 'Eden', 'Ob', 'take off', 'a lot', 'Hanukkah', 'rest', 'a cover']\n",
      "\n",
      "Topic 13: ['handicapped', 'loved', 'hairy', 'shit', 'fear', 'sixty', 'Flood', 'bath', 'Tool', 'Bad', 'Manasseh', 'came out', 'Lemech', 'Third', 'wipe', 'free', 'Boy', 'Beverage', 'No', 'Ken']\n",
      "\n",
      "Topic 14: ['comfortable', 'born', 'against', 'Foot', 'Ask', 'Closed', 'Third', 'carry', 'Goat', 'Ntsb', 'lest', 'water', 'its', 'Fraction', 'outside', 'read', 'Bowing', 'Maybe', 'laughed', 'Jesus']\n",
      "\n",
      "Topic 15: ['seventy', 'a tent', 'Rebecca', 'Ten', 'Yes', 'in the first place', 'B', 'Odon', 'pre', 'solution', 'dream', 'only', 'heart', 'mortal', 'hay', 'Blood', 'Snake', 'male', 'hairy', 'Pr']\n",
      "\n",
      "Topic 16: ['Hot', 'Hand', 'end', 'could', 'Doubtful', 'ten', 'Rachel', 'right', 'Moderator', 'shit', 'past', 'Lemech', 'answered', 'Costs', 'Abimelech', 'but', 'f', 'With', 'here', 'Arlah']\n",
      "\n",
      "Topic 17: ['star', 'Salary', 'head', 'Mother', 'Two', 'seven', 'put on', 'from', 'will be done', 'Gen', 'live', 'Canaan', 'Odon', 'Yahav', 'a dress', 'dreamed', 'comfortable', 'under', 'multi-', 'Oh no']\n",
      "\n",
      "Topic 18: ['Bad', 'birthright', 'ninety', 'full', 'food', 'Eden', 'Fell', 'left', 'cub', 'Heavy', 'put on', 'Efron', 'a day', 'Six', 'With', 'Sur', 'herd', 'See', 'plant', 'Yes']\n",
      "\n",
      "Topic 19: ['grass', 'Good', 'death', 'slave', 'Of money', 'who will', 'ten', 'Pr', 'dress', 'action', 'point', 'Desert', 'After all', 'Bad', 'Oil', 'Confusion', 'Hebrew', 'they', 'twenty', 'successful']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in topics:\n",
    "    topic_words = [GoogleTranslator(source='auto', target='en').translate(word[6:])\n",
    "                   for word in topic.split(' + ')]\n",
    "    print(f\"Topic {topic_id}: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a257d088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -79.45595529795459\n",
      "Average Coherence Score UCI: 3.3507918727205435\n"
     ]
    }
   ],
   "source": [
    "cumulative_coherence_umass = 0\n",
    "cumulative_coherence_uci = 0\n",
    "\n",
    "# Calculate coherence for each topic extracted by HDP\n",
    "for topic_num, topic in topics:\n",
    "    top_terms = [word[6:] for word in topic.split(' + ')]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, hebrew_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is similarly defined and imported\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, hebrew_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/len(topics))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644bd00",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37481d1",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "812fc94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['אמר', 'אֲשֶׁר', 'כֹּל', 'בֵּן', 'אֶרֶץ', 'כִּי', 'היה', 'אֱלֹהִים', 'אָב', 'לֹא']\n",
      "\n",
      "Topic 1: ['אמר', 'כֹּל', 'אֶרֶץ', 'אֲשֶׁר', 'היה', 'שָׁנָה', 'לֹא', 'כִּי', 'מִצְרַיִם', 'אָח']\n",
      "\n",
      "Topic 2: ['בֵּן', 'ילד', 'שָׁנָה', 'כֹּל', 'בַּת', 'אמר', 'מֵאָה', 'אֵלֶּה', 'שֵׁם', 'אִשָּׁה']\n",
      "\n",
      "Topic 3: ['אֲשֶׁר', 'שָׁנָה', 'היה', 'בֵּן', 'יוֺם', 'אֶרֶץ', 'מֵאָה', 'כִּי', 'לֹא', 'ילד']\n",
      "\n",
      "Topic 4: ['אֲשֶׁר', 'אֶרֶץ', 'שָׁנָה', 'בֵּן', 'היה', 'יוֺם', 'כִּי', 'עשׂה', 'אָב', 'אֱלֹהִים']\n",
      "\n",
      "Topic 0: ['said', 'which', 'all', 'son', 'country', 'Because', 'was', 'god', 'Ob', 'No']\n",
      "\n",
      "Topic 1: ['said', 'all', 'country', 'which', 'was', 'sleep', 'No', 'Because', 'Egypt', 'brother']\n",
      "\n",
      "Topic 2: ['son', 'Boy', 'sleep', 'all', 'a girl', 'said', 'century', 'goddess', 'Name', 'woman']\n",
      "\n",
      "Topic 3: ['which', 'sleep', 'was', 'son', 'a day', 'country', 'century', 'Because', 'No', 'Boy']\n",
      "\n",
      "Topic 4: ['which', 'country', 'sleep', 'son', 'was', 'a day', 'Because', 'action', 'Ob', 'god']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the HDP model\n",
    "num_topics = 5\n",
    "lsa_model = LsiModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "#Print the topics\n",
    "topics = lsa_model.show_topics(formatted=False)\n",
    "\n",
    "for topic_id, topic in topics:\n",
    "    topic_words = [word[0] for word in topic]\n",
    "    print(f\"Topic {topic_id}: {topic_words}\\n\")\n",
    "\n",
    "for topic_id, topic in topics:\n",
    "    topic_words = [GoogleTranslator(source='auto', target='en').translate(word[0])\n",
    "                    for word in topic]\n",
    "    print(f\"Topic {topic_id}: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "628f62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -411.41778097625985\n",
      "Average Coherence Score UCI: 15.571700900619172\n"
     ]
    }
   ],
   "source": [
    "for topic_num, topic in topics:\n",
    "    top_terms = [word[0] for word in topic]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, hebrew_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is similarly defined and imported\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, hebrew_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/len(topics))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bc3ba",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f403e5d0-b692-44cb-8839-00f0493e3d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1: אמר, ים, היה, יו, יש\n",
      "Component 2: ילד, חיה, ים, עו, הרה\n",
      "Component 3: היה, יו, מות, הו, וא\n",
      "Component 4: ים, עש, יו, מות, יהוה\n",
      "Component 5: יש, ים, לקח, נתן, הלך\n",
      "Component 6: יו, וא, יש, בר, ים\n",
      "Component 7: נתן, וא, יו, לקח, עש\n",
      "Component 8: וא, יהוה, הו, דו, ים\n",
      "Component 9: דו, הו, יהוה, יו, קרא\n",
      "Component 10: קרא, ראה, עו, יהוה, יצא\n",
      "Component 1: said, sea, was, Yu, there is\n",
      "Component 2: Boy, an animal, sea, O, pregnant\n",
      "Component 3: was, Yu, death, Oh, And\n",
      "Component 4: sea, moth, Yu, death, Jehovah\n",
      "Component 5: there is, sea, Lesson, Nathan, Gone\n",
      "Component 6: Yu, And, there is, bar, sea\n",
      "Component 7: Nathan, And, Yu, Lesson, moth\n",
      "Component 8: And, Jehovah, Oh, two, sea\n",
      "Component 9: two, Oh, Jehovah, Yu, read\n",
      "Component 10: read, See, O, Jehovah, came out\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Hebrew text\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "documents = [' '.join(word[1] for word in tokenizer.tokenize(line)) for line in hebrew_text.split('\\n')] \n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the documents into TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Define the number of components for LSA\n",
    "num_components = 10\n",
    "\n",
    "# Apply Truncated SVD to perform LSA\n",
    "lsa_model = TruncatedSVD(n_components=num_components)\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print the topics (components) and the top words for each component\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, component in enumerate(lsa_model.components_):\n",
    "    top_terms = [terms[term_idx] for term_idx in component.argsort()[:-6:-1]]\n",
    "    print(f\"Component {i+1}: {', '.join(top_terms)}\")\n",
    "\n",
    "\n",
    "for i, component in enumerate(lsa_model.components_):\n",
    "    top_terms = [GoogleTranslator(source='auto', target='en').translate(terms[term_idx] )\n",
    "                 for term_idx in component.argsort()[:-6:-1]]\n",
    "    print(f\"Component {i+1}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5138843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -12.763825758372425\n",
      "Average Coherence Score UCI: 2.5806430508707927\n"
     ]
    }
   ],
   "source": [
    "cumulative_coherence = 0\n",
    "cumulative_uci = 0\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_terms_idx = topic.argsort()[:-11:-1]\n",
    "    top_terms = [terms[idx] for idx in top_terms_idx]\n",
    "    cumulative_coherence += compute_coherence_score_umass(top_terms, hebrew_text_lines)\n",
    "    cumulative_uci += compute_coherence_score_uci(top_terms, hebrew_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence/len(lsa_model.components_))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_uci/len(lsa_model.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24274c",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186dc44b-e263-4daf-be9a-949914a40355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Topics:\n",
      "Topic 1: לקח הו ילד וא אמר\n",
      "Topic 2: מו או עו קו ים\n",
      "Topic 3: דו וא נתן עש יו\n",
      "Topic 4: ראה לקח נתן עש היה\n",
      "Topic 5: אש הו וא ית יש\n"
     ]
    }
   ],
   "source": [
    "# Apply NMF to the TF-IDF matrix\n",
    "nmf_model = NMF(n_components=num_topics)\n",
    "nmf_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print the topics\n",
    "print(\"NMF Topics:\")\n",
    "for i, component in enumerate(nmf_model.components_):\n",
    "    top_terms = [terms[j] for j in component.argsort()[-5:]]  # Top 5 terms per topic\n",
    "    print(f\"Topic {i+1}: {' '.join(top_terms)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
