{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af8b99c",
   "metadata": {},
   "source": [
    "# Topic Modeling on English Translation of Genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457b95b3-c0c8-47b5-835f-452eeed92907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import HdpModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary \n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from coherence.coherence_scores import compute_coherence_score_umass, compute_coherence_score_uci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29484016",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd6fbee-1e7b-46c5-9aba-e3af999ae48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Remove empty tokens and tokens with length less than 2\n",
    "    tokens = [token for token in tokens if token.strip() and len(token) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_dtm(text):\n",
    "    dtm = defaultdict(dict)\n",
    "    lines = text.strip().split('\\n')        \n",
    "    for idx, line in enumerate(lines):\n",
    "        # Treat each line as a separate document\n",
    "        verse = f\"Line {idx + 1}\"\n",
    "        verse_text = preprocess_text(line)\n",
    "        for word in verse_text:\n",
    "            if word in dtm[verse]:\n",
    "                dtm[verse][word] += 1\n",
    "            else:\n",
    "                dtm[verse][word] = 1\n",
    "                        \n",
    "    return dtm\n",
    "\n",
    "# Path to the English Genesis file\n",
    "eng_genesis_path = 'data/genesis_english.txt'\n",
    "with open(eng_genesis_path, 'r') as file:\n",
    "    eng_genesis_text = file.read()\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = ' '.join(preprocess_text(eng_genesis_text))\n",
    "\n",
    "# Create the Document-Term Matrix (DTM)\n",
    "dtm = create_dtm(eng_genesis_text)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = set()\n",
    "for verse in dtm:\n",
    "    for word in dtm[verse]:\n",
    "        vocab.add(word)\n",
    "\n",
    "#create numpy array of vocab\n",
    "terms = list(vocab)\n",
    "terms.sort()\n",
    "terms = np.array(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafc754",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "Assumes that each document is a mixture of topics and that each word in the document is attributable to one of the document's topics. The goal of LDA is to find the topics that best describe the collection of documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac741d3-4ea2-4756-8f5a-78adc07469f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['lord', 'said', 'shall', 'men', 'god', 'everi', 'brother', 'joseph', 'son', 'man']\n",
      "1 ['said', 'shall', 'god', 'son', 'jacob', 'day', 'father', 'abraham', 'lord', 'daughter']\n",
      "2 ['son', 'said', 'god', 'abraham', 'daughter', 'took', 'name', 'land', 'wife', 'year']\n",
      "3 ['said', 'came', 'land', 'father', 'lord', 'son', 'shall', 'jacob', 'god', 'brother']\n",
      "4 ['said', 'father', 'shall', 'behold', 'brother', 'son', 'lord', 'went', 'go', 'hous']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tokenized documents\n",
    "documents = [[word for word in word_counts.keys()] for word_counts in dtm.values()]\n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Apply LDA\n",
    "num_topics = 5\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.show_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    topic_words = [word[7:-1] for word in topic[1].split(' + ')]\n",
    "    print(topic[0], topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903e6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -105.47551766940516\n",
      "Average Coherence Score UCI: 2.1139594496717518\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_flat = preprocess_text(eng_genesis_text)\n",
    "preprocessed_text_lines = [preprocess_text(line) for line in eng_genesis_text.split('\\n')]\n",
    "\n",
    "\n",
    "# Example to compute UMass and UCI coherence for LDA topics\n",
    "cumulative_coherence_umass = 0\n",
    "cumulative_coherence_uci = 0\n",
    "\n",
    "for topic_idx, topic in lda_model.show_topics(formatted=False, num_topics=num_topics, num_words=10):\n",
    "    top_terms = [word for word, prob in topic]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, preprocessed_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is defined similarly to compute_coherence_score_umass\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, preprocessed_text_flat, window_size=10)\n",
    "\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/num_topics)\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df29974",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process (HDP)\n",
    "A non-parametric Bayesian method that allows the number of topics to be inferred from the data. It is an extension of LDA that infers the number of topics from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "876560a9-ad29-4c1c-97f7-078eb432d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['philistin', 'without', 'royal', 'laban', 'everlast', 'hamathit', 'preciou', 'camel', 'run', 'bound', 'endu', 'abomin', 'worth', 'hobah', 'name', 'stop', 'kept', 'garment', 'bethlehem', 'charg']\n",
      "1 ['lehabim', 'third', 'sweet', 'certain', 'canaan', 'tith', 'girgashit', 'accad', 'call', 'content', 'midwif', 'whale', 'rain', 'whomsoev', 'aner', 'thought', 'hundr', 'strip', 'sword', 'becom']\n",
      "2 ['vision', 'heat', 'help', 'give', 'drew', 'beriah', 'rephaim', 'ram', 'canaan', 'hid', 'wors', 'add', 'dinhabah', 'put', 'rode', 'merri', 'distress', 'refus', 'accept', 'land']\n",
      "3 ['deceit', 'fulfil', 'finish', 'seventeenth', 'abraham', 'wit', 'elbethel', 'readi', 'amraphel', 'twin', 'risen', 'ever', 'wittingli', 'fame', 'long', 'garment', 'statut', 'white', 'iniqu', 'beheld']\n",
      "4 ['stead', 'obtain', 'ziphion', 'magician', 'overtook', 'stand', 'breast', 'penuel', 'part', 'zepho', 'kiriathaim', 'alvan', 'ararat', 'break', 'heir', 'noon', 'bank', 'pildash', 'done', 'shearer']\n",
      "5 ['last', 'onan', 'omar', 'dove', 'mess', 'ehi', 'shuah', 'sheba', 'perizzit', 'heber', 'neither', 'becam', 'heat', 'syrian', 'scarc', 'chamber', 'barren', 'benjamin', 'drank', 'border']\n",
      "6 ['zilpah', 'worship', 'among', 'mehetabel', 'kneel', 'fourth', 'strength', 'heaven', 'ever', 'said', 'refrain', 'hurri', 'sevenfold', 'visit', 'hamul', 'king', 'heard', 'blood', 'bethuel', 'seba']\n",
      "7 ['beast', 'doubl', 'poor', 'shed', 'lead', 'loos', 'meat', 'pildash', 'mahalath', 'flee', 'leah', 'border', 'bodi', 'hold', 'heber', 'song', 'took', 'moabit', 'hurri', 'preciou']\n",
      "8 ['escap', 'dungeon', 'are', 'drew', 'made', 'quit', 'separ', 'whether', 'spare', 'meadow', 'hard', 'israel', 'nurs', 'grown', 'marri', 'face', 'mark', 'lightli', 'month', 'doubl']\n",
      "9 ['earth', 'done', 'close', 'gold', 'offer', 'seth', 'shekel', 'visit', 'bank', 'rais', 'moon', 'deni', 'karnaim', 'walk', 'midianit', 'know', 'dinhabah', 'golden', 'chamber', 'shinab']\n",
      "10 ['escap', 'butlership', 'comfort', 'rose', 'dishon', 'fast', 'order', 'confeder', 'pitcher', 'shalem', 'sheaf', 'corrupt', 'mahalaleel', 'ascend', 'betim', 'bela', 'saddl', 'aveng', 'verifi', 'ride']\n",
      "11 ['taken', 'shiloh', 'zimran', 'continu', 'weigh', 'naphtuhim', 'kohath', 'rebuk', 'attain', 'built', 'fed', 'trade', 'repent', 'hand', 'search', 'stone', 'submit', 'sister', 'wage', 'ladder']\n",
      "12 ['asenath', 'drought', 'casluhim', 'togarmah', 'serv', 'garden', 'told', 'philistin', 'rod', 'buz', 'simeon', 'strake', 'uppermost', 'reprov', 'drank', 'isl', 'need', 'pleasant', 'babel', 'bad']\n",
      "13 ['trade', 'mehetabel', 'esek', 'bind', 'nineteen', 'meet', 'unit', 'absent', 'shut', 'tith', 'instrument', 'trembl', 'knife', 'thirteenth', 'bashemath', 'unstabl', 'horn', 'content', 'meadow', 'gatam']\n",
      "14 ['gerar', 'drank', 'goe', 'circumcis', 'traffic', 'bethel', 'reviv', 'thu', 'moreov', 'herein', 'gutter', 'waxen', 'forth', 'sold', 'food', 'mischief', 'part', 'first', 'despis', 'greater']\n",
      "15 ['artific', 'fish', 'magician', 'rightli', 'wash', 'inquir', 'person', 'linen', 'dress', 'wittingli', 'quarter', 'judah', 'wickedli', 'hastili', 'lack', 'desol', 'instrument', 'egypt', 'whereof', 'meet']\n",
      "16 ['urg', 'seventeenth', 'kenaz', 'marvel', 'help', 'aros', 'long', 'strife', 'rank', 'travail', 'displeas', 'ruler', 'toward', 'bakemeat', 'maid', 'silver', 'fashion', 'sheav', 'jachin', 'castl']\n",
      "17 ['beeri', 'reproach', 'thahash', 'bozrah', 'shave', 'westward', 'coat', 'lift', 'aran', 'meat', 'mountain', 'tubalcain', 'fill', 'toward', 'strong', 'barren', 'mouth', 'friend', 'captain', 'seth']\n",
      "18 ['foolishli', 'softli', 'besid', 'elparan', 'laid', 'stay', 'heber', 'one', 'will', 'commit', 'jezer', 'loin', 'busi', 'uz', 'timnah', 'held', 'forgot', 'pill', 'pass', 'tebah']\n",
      "19 ['methusael', 'staff', 'smell', 'fig', 'ephrath', 'shed', 'bethlehem', 'wound', 'becher', 'sole', 'methuselah', 'laban', 'morsel', 'redeem', 'dig', 'ahuzzath', 'press', 'also', 'naked', 'captain']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess text\n",
    "preprocessed_text = [preprocess_text(verse_text) for verse_text in eng_genesis_text.split('\\n')]\n",
    "\n",
    "# Step 2: Create a dictionary and document-term matrix\n",
    "dictionary = Dictionary(preprocessed_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_text]\n",
    "\n",
    "# Step 3: Apply HDP model\n",
    "hdp_model = HdpModel(corpus, dictionary)\n",
    "\n",
    "# Get topics\n",
    "topics = hdp_model.show_topics()\n",
    "for topic in topics:\n",
    "    topic_words = [word[6:] for word in topic[1].split(' + ')]\n",
    "    print(topic[0], topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28a826c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -9.86607105025642\n",
      "Average Coherence Score UCI: 4.69697982093691\n"
     ]
    }
   ],
   "source": [
    "cumulative_coherence_umass = 0\n",
    "cumulative_coherence_uci = 0\n",
    "\n",
    "# Calculate coherence for each topic extracted by HDP\n",
    "for topic_num, topic in topics:\n",
    "    top_terms = [word[6:] for word in topic.split(' + ')]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, preprocessed_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is similarly defined and imported\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, preprocessed_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/len(topics))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c88d7574-88e8-43a9-b8e6-8ca4319ba740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to create word clouds for each topic\n",
    "# def create_wordclouds(hdp_model):\n",
    "#     topics = hdp_model.show_topics()\n",
    "#     for topic_id, topic in topics:\n",
    "#         word_freq = {word: float(freq) for freq, word in [pair.split('*') for pair in topic.split(' + ')]}\n",
    "#         wordcloud = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(word_freq)\n",
    "        \n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#         plt.title(f'Topic {topic_id} Word Cloud')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# # Function to plot topic distribution across documents\n",
    "# def plot_topic_distribution(hdp_model, corpus):\n",
    "#     topic_dist = hdp_model[corpus]\n",
    "#     topics = [dict(topic) for topic in topic_dist]\n",
    "#     topic_matrix = np.zeros((len(corpus), len(hdp_model.show_topics())))\n",
    "\n",
    "#     for doc_id, doc_topics in enumerate(topics):\n",
    "#         for topic_id, topic_prob in doc_topics.items():\n",
    "#             topic_matrix[doc_id][topic_id] = topic_prob\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.imshow(topic_matrix, aspect='auto', cmap='viridis')\n",
    "#     plt.colorbar(label='Probability')\n",
    "#     plt.title('Topic Distribution across Documents')\n",
    "#     plt.xlabel('Topic')\n",
    "#     plt.ylabel('Document')\n",
    "#     plt.show()\n",
    "\n",
    "# # Assuming `hdp_model` and `corpus` are already defined\n",
    "\n",
    "# # Visualize word clouds for each topic\n",
    "# create_wordclouds(hdp_model)\n",
    "\n",
    "# # Visualize topic distribution across documents\n",
    "# plot_topic_distribution(hdp_model, corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8bc0d",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)\n",
    "A technique in natural language processing, in particular in vectorial semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee212f55",
   "metadata": {},
   "source": [
    "###  Bag of Words\n",
    "A simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8735c30b-01fd-4764-82d8-d711f36c922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['said', 'son', 'shall', 'god', 'lord', 'father', 'brother', 'jacob', 'came', 'land']\n",
      "1 ['son', 'said', 'daughter', 'year', 'jacob', 'came', 'lord', 'hundr', 'shall', 'begat']\n",
      "2 ['son', 'came', 'land', 'said', 'joseph', 'brother', 'father', 'god', 'pass', 'egypt']\n",
      "3 ['shall', 'said', 'god', 'earth', 'jacob', 'everi', 'year', 'live', 'day', 'lord']\n",
      "4 ['shall', 'god', 'came', 'pass', 'year', 'lord', 'son', 'abraham', 'said', 'earth']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tokenized documents\n",
    "documents = [[word for word in word_counts.keys()] for word_counts in dtm.values()]\n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Apply LSA\n",
    "num_topics = 5\n",
    "lsa_model = models.LsiModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "# Print the topics\n",
    "topics = lsa_model.show_topics(num_topics=num_topics, num_words=10)\n",
    "for topic in topics:\n",
    "    topic_words = [word.split('\\\"')[1] for word in topic[1].split(' + ')]\n",
    "    print(topic[0], topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8267331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -100.18697587908224\n",
      "Average Coherence Score UCI: 2.14865509797958\n"
     ]
    }
   ],
   "source": [
    "cumulative_coherence_umass = 0\n",
    "cumulative_coherence_uci = 0\n",
    "\n",
    "# Calculate coherence for each topic extracted by HDP\n",
    "for topic_num, topic in topics:\n",
    "    top_terms = [word.split('\\\"')[1] for word in topic.split(' + ')]\n",
    "    cumulative_coherence_umass += compute_coherence_score_umass(top_terms, preprocessed_text_lines)\n",
    "    # Assuming compute_coherence_score_uci is similarly defined and imported\n",
    "    cumulative_coherence_uci += compute_coherence_score_uci(top_terms, preprocessed_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence_umass/len(topics))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_coherence_uci/len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72805db",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "A numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f49da87-0696-4307-9b2d-2bfc4fa9de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: said | daughter | name | hand | us | hous | come | earth | everi | esau\n",
      "Topic 1: shall | behold | daughter | earth | name | brother | go | esau | one | servant\n",
      "Topic 2: son | wife | isaac | call | everi | daughter | pass | abraham | earth | let\n",
      "Topic 3: god | man | behold | daughter | wife | brother | hous | lord | took | call\n",
      "Topic 4: lord | father | hand | wife | came | joseph | everi | isaac | pharaoh | went\n",
      "Topic 5: father | behold | say | brother | daughter | abraham | earth | took | us | name\n",
      "Topic 6: land | behold | abraham | brother | man | wife | also | let | name | jacob\n",
      "Topic 7: jacob | came | brother | behold | hand | say | us | egypt | day | took\n",
      "Topic 8: came | brother | man | behold | go | pass | abraham | daughter | year | bless\n",
      "Topic 9: brother | joseph | hand | name | pharaoh | went | day | also | wife | year\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess_text(eng_genesis_text)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Apply SVD to the TF-IDF matrix\n",
    "n_components = 10 # Number of topics\n",
    "lsa_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(X)\n",
    "\n",
    "# Print the top words for each topic and calculate coherence\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_terms_idx = topic.argsort()[:-11:-1]\n",
    "    top_terms = [terms[idx] for idx in top_terms_idx]\n",
    "    print(f\"Topic {i}: {' | '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc57d97c-b803-48fb-9a90-b666e284c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coherence Score UMASS: -120.18789376321706\n",
      "Average Coherence Score UCI: 1.9613278023196685\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text_flat = preprocess_text(eng_genesis_text)\n",
    "preprocessed_text_lines = [preprocess_text(line) for line in eng_genesis_text.split('\\n')]\n",
    "\n",
    "cumulative_coherence = 0\n",
    "cumulative_uci = 0\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_terms_idx = topic.argsort()[:-11:-1]\n",
    "    top_terms = [terms[idx] for idx in top_terms_idx]\n",
    "    cumulative_coherence += compute_coherence_score_umass(top_terms, preprocessed_text_lines)\n",
    "    cumulative_uci += compute_coherence_score_uci(top_terms, preprocessed_text_flat, window_size=10)\n",
    "\n",
    "print(\"Average Coherence Score UMASS:\", cumulative_coherence/len(lsa_model.components_))\n",
    "print(\"Average Coherence Score UCI:\", cumulative_uci/len(lsa_model.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7876b",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "A group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. In topic modeling, the matrix V represents the documents, W represents the topics, and H represents the weights of the topics in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06a1a40a-bd6b-48fc-a4fa-de17288844fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Apply NMF to the TF-IDF matrix\n",
    "n_components = 10  # Number of topics\n",
    "nmf_model = NMF(n_components=n_components, random_state=42)\n",
    "nmf_topic_matrix = nmf_model.fit_transform(X)\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print topics, tokens, and corresponding vectors\n",
    "# print(\"Topics, tokens, and corresponding vectors:\")\n",
    "# for i, topic_vector in enumerate(nmf_model.components_):\n",
    "#     print(f\"Topic {i+1}:\")\n",
    "#     for token, weight in zip(feature_names, topic_vector):\n",
    "#         print(f\"{token}: {weight}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28a6f4ae-a334-4654-8ef7-8b3d2331b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "said, took, isaac, one, egypt, daughter, made, live, give, begat\n",
      "Topic 2:\n",
      "shall, day, abraham, year, earth, went, behold, hand, call, daughter\n",
      "Topic 3:\n",
      "son, day, joseph, year, behold, abraham, went, earth, hand, man\n",
      "Topic 4:\n",
      "god, joseph, day, man, behold, abraham, year, daughter, earth, went\n",
      "Topic 5:\n",
      "lord, joseph, day, abraham, year, hand, name, went, earth, man\n",
      "Topic 6:\n",
      "father, joseph, abraham, day, behold, earth, daughter, year, say, name\n",
      "Topic 7:\n",
      "land, joseph, abraham, behold, day, man, year, earth, name, wife\n",
      "Topic 8:\n",
      "jacob, joseph, day, behold, abraham, hand, went, say, year, earth\n",
      "Topic 9:\n",
      "came, abraham, man, behold, joseph, day, daughter, year, earth, say\n",
      "Topic 10:\n",
      "brother, joseph, day, year, name, hand, went, abraham, wife, call\n"
     ]
    }
   ],
   "source": [
    "    # Print the topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        top_idx = topic.argsort()[:-11:-1]\n",
    "        top_terms = [feature_names[i] for i in top_idx]\n",
    "        print(\", \".join(top_terms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
